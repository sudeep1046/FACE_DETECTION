# -*- coding: utf-8 -*-
"""FaceDetection01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IHCxNpYa62SvevqCGkdra1I4T8VbEh8-
"""

!pip install gtts
!pip install streamlit
!pip install pyngrok

!pip install mediapipe
import streamlit as st
from IPython.display import display, Javascript, Image
from google.colab.output import eval_js
from base64 import b64decode, b64encode
import cv2
import numpy as np
import PIL
import io
import time
import mediapipe as mp  # Add MediaPipe for hand detection
from gtts import gTTS   # Import gTTS for voice
from google.colab import output  # To play audio

mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.7)
mp_drawing = mp.solutions.drawing_utils

def speak_text(text):
    tts = gTTS(text)
    tts.save("output.mp3")
    output.eval_js('new Audio("output.mp3").play()')

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import cv2
# import numpy as np
# import mediapipe as mp
# from gtts import gTTS
# from io import BytesIO
# from PIL import Image
# 
# mp_hands = mp.solutions.hands
# mp_drawing = mp.solutions.drawing_utils
# mp_holistic = mp.solutions.holistic
# 
# hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.7)
# face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
# 
# def process_image(image):
#     image = np.array(image.convert('RGB'))
#     image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
#     faces = face_cascade.detectMultiScale(gray, 1.1, 4)
#     for (x, y, w, h) in faces:
#         cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)
#     results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
#     if results.multi_hand_landmarks:
#         for hand_landmarks in results.multi_hand_landmarks:
#             mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)
#     return image, len(faces), len(results.multi_hand_landmarks) if results.multi_hand_landmarks else 0
# 
# def speak_text(text):
#     tts = gTTS(text)
#     audio_bytes = BytesIO()
#     tts.save(audio_bytes)
#     st.audio(audio_bytes.getvalue(), format='audio/mp3')
# 
# st.title("Hand and Face Detection App")
# uploaded_file = st.file_uploader("Choose an image...", type="jpg")
# if uploaded_file is not None:
#     image = Image.open(uploaded_file)
#     processed_image, face_count, hand_count = process_image(image)
#     st.image(image, caption='Uploaded Image.', use_column_width=True)
#     st.image(processed_image, caption='Processed Image.', use_column_width=True)
#     st.write(f"Detected {face_count} face(s) and {hand_count} hand(s).")
#     if st.button("Announce Detection"):
#         speak_text(f"{face_count} face{'s' if face_count != 1 else ''} and {hand_count} hand{'s' if hand_count != 1 else ''} detected.")

!pip install Pillow
import io
import PIL.Image
import numpy as np

def bbox_to_bytes(bbox_array):
    """
    Encode a bounding box array as a base64 image byte string.

    Args:
           bbox_array: Numpy array with bounding boxes

    Returns:
           bytes: Base64 image byte string
    """

    # Ensure the array has the correct shape and data type
    if len(bbox_array.shape) == 2:
        bbox_array = np.stack((bbox_array,)*3, axis=-1) # Convert to 3 channels if grayscale
    bbox_array = bbox_array.astype(np.uint8)

    # Add an alpha channel if missing
    if bbox_array.shape[2] == 3:
        alpha_channel = np.ones(bbox_array.shape[:2], dtype=np.uint8) * 255
        bbox_array = np.dstack((bbox_array, alpha_channel))

    bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')
    iobuf = io.BytesIO()
    bbox_PIL.save(iobuf, format='png')
    bbox_bytes = 'data:image/png;base64,' + base64.b64encode(iobuf.getvalue()).decode('utf-8')
    return bbox_bytes

def js_to_image(js_reply):
    """
    Params:
            js_reply: JavaScript object containing image from webcam
    Returns:
            img: OpenCV BGR image
    """
    image_bytes = b64decode(js_reply.split(',')[1])
    jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)
    img = cv2.imdecode(jpg_as_np, flags=1)
    return img

def bbox_to_bytes(bbox_array):
    """
    Params:
            bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.
    Returns:
          bytes: Base64 image byte string
    """
    bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')
    iobuf = io.BytesIO()
    bbox_PIL.save(iobuf, format='png')
    bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))
    return bbox_bytes

face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))

def take_photo(filename='photo.jpg', quality=0.8):
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();


      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);


      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
  display(js)


  data = eval_js('takePhoto({})'.format(quality))

  img = js_to_image(data)

  gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
  print(gray.shape)

  faces = face_cascade.detectMultiScale(gray)

  for (x,y,w,h) in faces:
      img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)

  cv2.imwrite(filename, img)

  return filename

try:
  filename = take_photo('photo.jpg')
  print('Saved to {}'.format(filename))

  # Show the image which was just taken.
  display(Image(filename))
except Exception as err:
  # Errors will be thrown if the user does not have a webcam or if they do not
  # grant the page permission to access it.
  print(str(err))

def video_stream():
    js = Javascript('''
      var video;
      var div = null;
      var stream;
      var captureCanvas;
      var imgElement;
      var labelElement;

      var pendingResolve = null;
      var shutdown = false;

      function removeDom() {
         stream.getVideoTracks()[0].stop();
         video.remove();
         div.remove();
         video = null;
         div = null;
         stream = null;
         imgElement = null;
         captureCanvas = null;
         labelElement = null;
      }

      function onAnimationFrame() {
        if (!shutdown) {
          window.requestAnimationFrame(onAnimationFrame);
        }
        if (pendingResolve) {
          var result = "";
          if (!shutdown) {
            captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);
            result = captureCanvas.toDataURL('image/jpeg', 0.8)
          }
          var lp = pendingResolve;
          pendingResolve = null;
          lp(result);
        }
      }

      async function createDom() {
        if (div !== null) {
          return stream;
        }

        div = document.createElement('div');
        div.style.border = '2px solid black';
        div.style.padding = '3px';
        div.style.width = '100%';
        div.style.maxWidth = '600px';
        document.body.appendChild(div);

        const modelOut = document.createElement('div');
        modelOut.innerHTML = "<span>Status:</span>";
        labelElement = document.createElement('span');
        labelElement.innerText = 'No data';
        labelElement.style.fontWeight = 'bold';
        modelOut.appendChild(labelElement);
        div.appendChild(modelOut);

        video = document.createElement('video');
        video.style.display = 'block';
        video.width = div.clientWidth - 6;
        video.setAttribute('playsinline', '');
        video.onclick = () => { shutdown = true; };
        stream = await navigator.mediaDevices.getUserMedia(
            {video: { facingMode: "environment"}});
        div.appendChild(video);

        imgElement = document.createElement('img');
        imgElement.style.position = 'absolute';
        imgElement.style.zIndex = 1;
        imgElement.onclick = () => { shutdown = true; };
        div.appendChild(imgElement);

        const instruction = document.createElement('div');
        instruction.innerHTML =
            '<span style="color: red; font-weight: bold;">' +
            'When finished, click here or on the video to stop this demo</span>';
        div.appendChild(instruction);
        instruction.onclick = () => { shutdown = true; };

        video.srcObject = stream;
        await video.play();

        captureCanvas = document.createElement('canvas');
        captureCanvas.width = 640; //video.videoWidth;
        captureCanvas.height = 480; //video.videoHeight;
        window.requestAnimationFrame(onAnimationFrame);

        return stream;
      }
      async function stream_frame(label, imgData) {
        if (shutdown) {
          removeDom();
          shutdown = false;
          return '';
        }

        var preCreate = Date.now();
        stream = await createDom();

        var preShow = Date.now();
        if (label != "") {
          labelElement.innerHTML = label;
        }

        if (imgData != "") {
          var videoRect = video.getClientRects()[0];
          imgElement.style.top = videoRect.top + "px";
          imgElement.style.left = videoRect.left + "px";
          imgElement.style.width = videoRect.width + "px";
          imgElement.style.height = videoRect.height + "px";
          imgElement.src = imgData;
        }

        var preCapture = Date.now();
        var result = await new Promise(function(resolve, reject) {
          pendingResolve = resolve;
        });
        shutdown = false;

        return {'create': preShow - preCreate,
                'show': preCapture - preShow,
                'capture': Date.now() - preCapture,
                'img': result};
      }
      ''')

    display(js)

def video_frame(label, bbox):
    data = eval_js('stream_frame("{}", "{}")'.format(label, bbox))
    return data

from gtts import gTTS
from IPython.display import Audio, display
import os

# Variables to track previously announced counts
prev_face_count = 0
prev_hand_count = 0

def speak_text(text):
    """Convert text to speech and play audio in the notebook."""
    tts = gTTS(text=text, lang='en')
    filename = "voice.mp3"
    tts.save(filename)

    # Play the saved audio file directly in the notebook
    display(Audio(filename, autoplay=True))

    # Clean up by removing the file after playing
    os.remove(filename)

video_stream()

label_html = 'Capturing...'

bbox = ''
count = 0

while True:
    js_reply = video_frame(label_html, bbox)
    if not js_reply:
        break

    img = js_to_image(js_reply["img"])
    bbox_array = np.zeros([480, 640, 4], dtype=np.uint8)

    # Convert the image to RGB for both face and hand recognition
    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Face detection
    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    faces = face_cascade.detectMultiScale(gray)

    face_count = len(faces)

    # Add a label for each detected face
    for idx, (x, y, w, h) in enumerate(faces, 1):
        bbox_array = cv2.rectangle(bbox_array, (x, y), (x + w, y + h), (255, 0, 0), 2)
        label = f"Face {idx}"
        cv2.putText(bbox_array, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)

    # Hand detection using MediaPipe
    hand_results = hands.process(rgb_img)

    hand_count = 0
    if hand_results.multi_hand_landmarks:
        hand_count = len(hand_results.multi_hand_landmarks)
        for hand_idx, hand_landmarks in enumerate(hand_results.multi_hand_landmarks):
            # Get bounding box around the hand
            hand_xs = [landmark.x for landmark in hand_landmarks.landmark]
            hand_ys = [landmark.y for landmark in hand_landmarks.landmark]
            hand_min_x = int(min(hand_xs) * 640)
            hand_max_x = int(max(hand_xs) * 640)
            hand_min_y = int(min(hand_ys) * 480)
            hand_max_y = int(max(hand_ys) * 480)

            # Draw green rectangle around the hand
            bbox_array = cv2.rectangle(bbox_array, (hand_min_x, hand_min_y), (hand_max_x, hand_max_y), (0, 255, 0), 2)

            hand_label = f"Hand {hand_idx + 1}"
            cv2.putText(bbox_array, hand_label, (hand_min_x, hand_min_y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

    bbox_array[:, :, 3] = (bbox_array.max(axis=2) > 0).astype(int) * 255

    bbox_bytes = bbox_to_bytes(bbox_array)
    bbox = bbox_bytes

    # Label the detected objects in HTML
    label_html = f"{face_count} Face(s), {hand_count} Hand(s) Detected"

    # Announce only if face or hand count has changed
    if face_count != prev_face_count or hand_count != prev_hand_count:
        # Generate voice output for face and hand count changes
        announcement = ""
        if face_count != prev_face_count:
            announcement += f"{face_count} face{'s' if face_count != 1 else ''} detected. "
        if hand_count != prev_hand_count:
            announcement += f"{hand_count} hand{'s' if hand_count != 1 else ''} detected."

        if announcement:
            speak_text(announcement)

        # Update previous counts to avoid repeating announcements
        prev_face_count = face_count
        prev_hand_count = hand_count